{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32708da-aa07-4ceb-9ccc-7fadedbc8a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nazan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nazan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nazan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install seaborn\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f57e69-a737-4cab-92ef-63382c4b65ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 235277\n",
      "Number of columns: 60\n"
     ]
    }
   ],
   "source": [
    "output=pd.read_csv('data_with_internal_info.csv', low_memory=False)\n",
    "rows, columns = output.shape\n",
    "print(\"Number of rows:\", rows)\n",
    "print(\"Number of columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db033a9-c787-4e8f-bea3-7c4514ca14bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nazan\\appdata\\roaming\\python\\python312\\site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nazan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c66658-7bef-432b-9556-1a7b752df7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nazan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nazan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\nazan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation for Compas Grade (TF-IDF + SVD + XGBoost) ---\n",
      "MAE:  0.3291\n",
      "RMSE: 0.4770\n",
      "R²:   0.9729\n",
      "MAPE: 0.0304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "# Prepare stopwords and lemmatizer\n",
    "_stopwords = set(stopwords.words(\"english\"))\n",
    "_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_prep(text_list):\n",
    "    cleaned = []\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for doc in text_list:\n",
    "        doc = str(doc).lower().translate(table)\n",
    "        doc = doc.replace('\\n', ' ')\n",
    "        doc = re.sub(r'[^a-z0-9._\\s]', '', doc)\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "        tokens = [tok for tok in doc.split() if tok not in _stopwords and len(tok) > 2]\n",
    "        lemmas = [_lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "        cleaned.append(\" \".join(lemmas))\n",
    "    return cleaned\n",
    "columns_to_combine = [\n",
    "    'Job Title (en)', 'Job Title (nl)', 'Domain (en)', 'Specialisation (en)',\n",
    "    'Career ladder en', 'Level Title', 'Function goal',\n",
    "    'Key result areas: result area (1)', 'Key result areas: result area (2)',\n",
    "    'Key result areas: result area (3)', 'Key result areas: result area (4)',\n",
    "    'Key result areas: result area (5)', 'Key result areas: result area (6)',\n",
    "    'Key result areas: result area (7)', 'Key result areas: result area (8)',\n",
    "    'Key result areas: result area (9)', 'Key result areas: result area (10)',\n",
    "    'Key result areas: result area (11)', 'Key result areas: result area (12)',\n",
    "    'Key result areas: result area (13)', 'Key result areas: result area (14)',\n",
    "    'Key result areas: result area (15)', 'HIERARCHIC MANAGEMENT',\n",
    "    'FUNCTIONAL MANAGEMENT', 'Leadership | Manage the following job(s)',\n",
    "    'Managed by', 'Is he/ she responsible for a certain budget/ figure?',\n",
    "    'Specify the budget amounts.', 'Diploma', 'Speciality',\n",
    "    'Required experience', 'Innovation', 'Row 4 - Column 1',\n",
    "    'Row 5 - Column 1', 'Row 6 - Column 1', 'Row 7 - Column 1',\n",
    "    'Row 8 - Column 1', 'Row 9 - Column 1', 'Reference Job', 'Age',\n",
    "    'Diploma Category', 'Job Level', 'Internal Job', 'Internal Job Grade',\n",
    "    'Department'\n",
    "]\n",
    "output[columns_to_combine] = output[columns_to_combine].astype(str)\n",
    "text_combined = output[columns_to_combine].agg(' '.join, axis=1).tolist()\n",
    "text_cleaned = text_prep(text_combined)\n",
    "\n",
    "# Text pipeline: TF-IDF + SVD\n",
    "def identity_preprocessor(x):\n",
    "    return x\n",
    "\n",
    "def whitespace_tokenizer(x):\n",
    "    return x.split()\n",
    "text_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        max_features=2000,\n",
    "        sublinear_tf=True,\n",
    "        smooth_idf=True,\n",
    "        stop_words='english',\n",
    "        tokenizer=whitespace_tokenizer,\n",
    "        preprocessor=identity_preprocessor\n",
    "    )),\n",
    "    ('svd', TruncatedSVD(n_components=100, random_state=11))\n",
    "])\n",
    "\n",
    "\n",
    "# Fit & transform text\n",
    "X_text = text_pipe.fit_transform(text_cleaned)\n",
    "X_text = pd.DataFrame(X_text, index=output.index)\n",
    "\n",
    "\n",
    "X = X_text\n",
    "\n",
    "y = output['Compas Grade'].fillna(output['Compas Grade'].mean())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=11,)\n",
    "\n",
    "# XGBoost regressor\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=11\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"--- Evaluation for Compas Grade (TF-IDF + SVD + XGBoost) ---\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {mean_squared_error(y_test, y_pred) ** 0.5:.4f}\")\n",
    "print(f\"R²:   {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, y_pred):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fdddd2-7a34-4651-b7a1-17135f46f427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   2.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=0.6; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=0.6; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=0.6; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=1.0; total time=   7.1s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=100, subsample=1.0; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.6; total time=  21.5s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.6; total time=  20.6s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=0.6; total time=  22.1s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=1.0; total time=  21.4s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=1.0; total time=  21.4s\n",
      "[CV] END learning_rate=0.01, max_depth=8, n_estimators=300, subsample=1.0; total time=  21.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.6; total time=   7.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.6; total time=   7.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.6; total time=   7.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.3s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=0.6; total time=   8.4s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=0.6; total time=   8.3s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=1.0; total time=   8.3s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=1.0; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=100, subsample=1.0; total time=   8.2s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=0.6; total time=  17.4s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=0.6; total time=  18.2s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=0.6; total time=  19.4s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=1.0; total time=  17.1s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=1.0; total time=  17.6s\n",
      "[CV] END learning_rate=0.05, max_depth=8, n_estimators=300, subsample=1.0; total time=  17.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   9.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   9.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   8.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   7.9s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=0.6; total time=   8.4s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=0.6; total time=   7.1s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=0.6; total time=   7.8s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=0.6; total time=  17.2s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=0.6; total time=  19.7s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=0.6; total time=  23.5s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=1.0; total time=  18.6s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=1.0; total time=  19.1s\n",
      "[CV] END learning_rate=0.1, max_depth=8, n_estimators=300, subsample=1.0; total time=  15.0s\n",
      "Best hyperparameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 300, 'subsample': 0.6}\n",
      "\n",
      "--- Evaluation for Compas Grade with XGBoost (tuned) ---\n",
      "  MAE:  0.18867690861225128\n",
      "  RMSE: 0.328625399633914\n",
      "  R²:   0.9871370196342468\n",
      "  MAPE: 0.016445359215140343\n"
     ]
    }
   ],
   "source": [
    "#Gridsearch on xgboost\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',     \n",
    "    verbosity=1,\n",
    "    random_state=11)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 1.0]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    model_xgb,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters for XGBoost:\", grid_search.best_params_)\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "y_true = y_test.values.flatten()\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation for Compas Grade with XGBoost (tuned) ---\")\n",
    "print(\"  MAE: \", mean_absolute_error(y_true, y_pred))\n",
    "print(\"  RMSE:\", mean_squared_error(y_true, y_pred)**0.5)\n",
    "print(\"  R²:  \", r2_score(y_true, y_pred))\n",
    "print(\"  MAPE:\", mean_absolute_percentage_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3f5f9c-ad19-4c89-b1dd-bb12ab12bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for Grade (Random Forest regressors with TF-IDF) ---\n",
      "  MAE: 0.7067727653933432\n",
      "  RMSE: 1.0187606054844331\n",
      "  R²: 0.8763815855091328\n",
      "  MAPE: 0.07113055287210276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#base model of random forest\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=11,\n",
    "    n_jobs=-1\n",
    ")\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test .columns = X_test .columns.astype(str)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n--- Evaluation for Grade (Random Forest regressors with TF-IDF) ---\")\n",
    "print(\"  MAE:\", mean_absolute_error(y_test, y_pred_rf))\n",
    "print(\"  RMSE:\", mean_squared_error(y_test, y_pred_rf) ** 0.5)\n",
    "print(\"  R²:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"  MAPE:\", mean_absolute_percentage_error(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97acb96a-2102-4be4-8739-4dcb8847eb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=50; total time=   2.9s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=50; total time=   3.2s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=100; total time=   6.6s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=100; total time=   6.5s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=2, n_estimators=100; total time=   6.4s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=50; total time=   3.2s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=   5.5s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=   5.4s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  10.7s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  10.5s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  10.5s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  11.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=2, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=  11.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_split=5, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  19.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  20.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=2, n_estimators=100; total time=  21.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=  10.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=   9.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=50; total time=   9.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  18.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  17.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_split=5, n_estimators=100; total time=  18.8s\n",
      "Best hyperparameters for RandomForest: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "--- Evaluation for Grade (Random Forest(Tuned) with TF-IDF) ---\n",
      "  MAE:  0.30466739381977753\n",
      "  RMSE: 0.45049945201898506\n",
      "  R²:   0.9758271744704401\n",
      "  MAPE: 0.027872298409766665\n"
     ]
    }
   ],
   "source": [
    "#-------GridSearch for random forest \n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth':    [ 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# 4) Set up GridSearchCV\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=model_rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5) Fit the grid search\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# 6) Grab best estimator and print best params\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"Best hyperparameters for RandomForest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# 7) Evaluate on test set\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation for Grade (Random Forest(Tuned) with TF-IDF) ---\")\n",
    "print(\"  MAE: \", mean_absolute_error(y_test, y_pred_rf))\n",
    "print(\"  RMSE:\", mean_squared_error(y_test, y_pred_rf) ** 0.5)\n",
    "print(\"  R²:  \", r2_score(y_test, y_pred_rf))\n",
    "print(\"  MAPE:\", mean_absolute_percentage_error(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c88b66-fd75-4a47-8fa2-96442bd2803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "save_dir = \"models/Grade\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save XGBoost model\n",
    "best_xgb.save_model(f\"{save_dir}/salary_xgboost.json\")\n",
    "\n",
    "# 2. Save text pipeline (TF-IDF + SVD)\n",
    "joblib.dump(text_pipe, f\"{save_dir}/text_pipeline.pkl\")\n",
    "\n",
    "# 3. Save metadata\n",
    "joblib.dump(columns_to_combine, f\"{save_dir}/columns_to_combine.pkl\")\n",
    "\n",
    "print(\"Grade model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35aaaf-bf8e-4761-b0cf-fa88c184fca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
